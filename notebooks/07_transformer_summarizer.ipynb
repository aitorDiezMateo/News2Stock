{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c94bb1",
   "metadata": {},
   "source": [
    "# Transformer Architecture for Summarization Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62beb7ba",
   "metadata": {},
   "source": [
    "## Importar las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2837140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87534e9",
   "metadata": {},
   "source": [
    "## Cargar y dividir el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3737d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"../data/news/summarized/apple_news.parquet\")\n",
    "data = data[['clean_body', 'body_summary']].dropna()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "871c8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "val_data = data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1fd2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (19366, 3)\n",
      "Test data shape: (4842, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data shape: {train_data.shape}\"\n",
    "      f\"\\nTest data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9d4f5",
   "metadata": {},
   "source": [
    "## Modelo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2955b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_len: int = 5000, dropout: float = 0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, \n",
    "                                     V: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = self.softmax(scores)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        ff_output = self.dropout(ff_output)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \n",
    "        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        self_attn_output = self.dropout(self_attn_output)\n",
    "        x = self.norm1(x + self_attn_output)\n",
    "        \n",
    "        cross_attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
    "        cross_attn_output = self.dropout(cross_attn_output)\n",
    "        x = self.norm2(x + cross_attn_output)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        ff_output = self.dropout(ff_output)\n",
    "        x = self.norm3(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, num_layers: int, dropout: float = 0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "                                     for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, num_layers: int, dropout: float = 0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "                                     for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "074ccb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 512, num_heads: int = 8, \n",
    "                 d_ff: int = 2048, num_encoder_layers: int = 6, num_decoder_layers: int = 6, \n",
    "                 dropout: float = 0.1, max_seq_len: int = 1024, pad_token_id: int = 0):\n",
    "        super(TransformerSummarizer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
    "        \n",
    "        self.src_positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        self.tgt_positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_encoder_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_decoder_layers, dropout)\n",
    "        \n",
    "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def create_mask(self, src: torch.Tensor, tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        src_mask = (src != self.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        src_mask = src_mask.to(dtype=torch.float)\n",
    "        \n",
    "        tgt_mask = (tgt != self.pad_token_id).unsqueeze(1).unsqueeze(2).float()\n",
    "        seq_len = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len, device=tgt.device), diagonal=1)).to(dtype=torch.float)\n",
    "        tgt_mask = tgt_mask * nopeak_mask\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        src_mask, tgt_mask = self.create_mask(src, tgt)\n",
    "        \n",
    "        src_embed = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_embed = self.src_positional_encoding(src_embed)\n",
    "        encoder_output = self.encoder(src_embed, src_mask)\n",
    "        \n",
    "        tgt_embed = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_embed = self.tgt_positional_encoding(tgt_embed)\n",
    "        decoder_output = self.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.output_linear(decoder_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerSummarizerTrainer:\n",
    "    def __init__(self, model: TransformerSummarizer, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    def train_epoch(self, dataloader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "        for src, tgt, tgt_y in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "            tgt_y = tgt_y.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            output = self.model(src, tgt)\n",
    "            loss = self.criterion(output.reshape(-1, output.size(-1)), tgt_y.reshape(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    def val_epoch(self, dataloader) -> float:\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for src, tgt, tgt_y in pbar:\n",
    "                src = src.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                tgt_y = tgt_y.to(self.device)\n",
    "                \n",
    "                output = self.model(src, tgt)\n",
    "                loss = self.criterion(output.reshape(-1, output.size(-1)), tgt_y.reshape(-1))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    def fit(self, train_dataloader, val_dataloader, epochs: int = 50, early_stopping_patience: int = 10):\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        pbar = tqdm(range(epochs), desc=\"Epochs\")\n",
    "        for epoch in pbar:\n",
    "            train_loss = self.train_epoch(train_dataloader)\n",
    "            val_loss = self.val_epoch(val_dataloader)\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            pbar.set_postfix({'train_loss': f'{train_loss:.4f}', 'val_loss': f'{val_loss:.4f}'})\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                self.save_checkpoint('best_model.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    pbar.close()\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "    \n",
    "    def generate_summary(self, src: torch.Tensor, max_len: int = 100, \n",
    "                        start_token_id: int = 1, end_token_id: int = 2) -> torch.Tensor:\n",
    "        self.model.eval()\n",
    "        \n",
    "        src = src.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).float()\n",
    "        src_embed = self.model.src_embedding(src) * math.sqrt(self.model.d_model)\n",
    "        src_embed = self.model.src_positional_encoding(src_embed)\n",
    "        encoder_output = self.model.encoder(src_embed, src_mask)\n",
    "        \n",
    "        tgt = torch.tensor([[start_token_id]], device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_len):\n",
    "                tgt_mask = torch.ones(1, tgt.size(1), tgt.size(1), device=self.device)\n",
    "                tgt_mask = torch.tril(tgt_mask)\n",
    "                \n",
    "                tgt_embed = self.model.tgt_embedding(tgt) * math.sqrt(self.model.d_model)\n",
    "                tgt_embed = self.model.tgt_positional_encoding(tgt_embed)\n",
    "                decoder_output = self.model.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "                \n",
    "                output = self.model.output_linear(decoder_output)\n",
    "                next_token = output[0, -1, :].argmax(dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                tgt = torch.cat([tgt, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == end_token_id:\n",
    "                    break\n",
    "        \n",
    "        return tgt.squeeze(0)\n",
    "    \n",
    "    def save_checkpoint(self, filepath: str):\n",
    "        torch.save({\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'history': self.history\n",
    "        }, filepath)\n",
    "    \n",
    "    def load_checkpoint(self, filepath: str):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        self.history = checkpoint['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c92f7c",
   "metadata": {},
   "source": [
    "## Prepare the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb4a8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_src_len=512, max_tgt_len=128):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens = self.tokenizer.encode(self.texts[idx])[:self.max_src_len]\n",
    "        tgt_tokens = self.tokenizer.encode(self.summaries[idx])[:self.max_tgt_len - 1]\n",
    "            \n",
    "        src_pad_len = self.max_src_len - len(src_tokens)\n",
    "        tgt_pad_len = self.max_tgt_len - len(tgt_tokens) - 1\n",
    "            \n",
    "        src = torch.tensor(src_tokens + [0] * src_pad_len, dtype=torch.long)\n",
    "        tgt = torch.tensor([1] + tgt_tokens + [0] * tgt_pad_len, dtype=torch.long)\n",
    "        tgt_y = torch.tensor(tgt_tokens + [2] + [0] * tgt_pad_len, dtype=torch.long)\n",
    "            \n",
    "        return src, tgt, tgt_\n",
    "    \n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = {}\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "            \n",
    "        sorted_words = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, _) in enumerate(sorted_words[:self.vocab_size - 4], start=4):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            \n",
    "        self.word2idx['<PAD>'] = 0\n",
    "        self.word2idx['<START>'] = 1\n",
    "        self.word2idx['<END>'] = 2\n",
    "        self.word2idx['<UNK>'] = 3\n",
    "        \n",
    "    def encode(self, text):\n",
    "        words = text.lower().split()\n",
    "        return [self.word2idx.get(word, 3) for word in words]\n",
    "        \n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.idx2word.get(idx, '<UNK>') for idx in tokens if idx > 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22070fc6",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "282dec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bac85e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer(vocab_size=10000)\n",
    "tokenizer.build_vocab(pd.concat([train_data['clean_body'], train_data['body_summary']]).tolist())\n",
    "    \n",
    "train_dataset = SummarizationDataset(\n",
    "    train_data['clean_body'].tolist(),\n",
    "    train_data['body_summary'].tolist(),\n",
    "    tokenizer,\n",
    "    max_src_len=512,\n",
    "    max_tgt_len=128\n",
    ")\n",
    "    \n",
    "val_dataset = SummarizationDataset(\n",
    "    val_data['clean_body'].tolist(),\n",
    "    val_data['body_summary'].tolist(),\n",
    "    tokenizer,\n",
    "    max_src_len=512,\n",
    "    max_tgt_len=128\n",
    ")\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "051cfceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSummarizer(\n",
    "    vocab_size=10000,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    d_ff=1024,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=512\n",
    ")\n",
    "trainer = TransformerSummarizerTrainer(model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b67675",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (2048) to match target batch_size (2064).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 109\u001b[0m, in \u001b[0;36mTransformerSummarizerTrainer.fit\u001b[0;34m(self, train_dataloader, val_dataloader, epochs, early_stopping_patience)\u001b[0m\n\u001b[1;32m    106\u001b[0m patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 109\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_epoch(val_dataloader)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[51], line 75\u001b[0m, in \u001b[0;36mTransformerSummarizerTrainer.train_epoch\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     74\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(src, tgt)\n\u001b[0;32m---> 75\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     78\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/loss.py:1297\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2048) to match target batch_size (2064)."
     ]
    }
   ],
   "source": [
    "trainer.fit(train_loader, val_loader, epochs=20, early_stopping_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8de78a",
   "metadata": {},
   "source": [
    "## Test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e957f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample 1]\n",
      "Original Text: Investors are quick to react to any and all news relating to Apple Inc. (NASDAQ: AAPL ), but there are times when they might want to ignore the headli...\n",
      "True Summary: Weekly take on tech news from Benzinga: Apple Watch, iPad sales expectations, and more. (Closed): Some of the best tech stories from the past seven days.) .. In Case You Missed It: A round-up of interesting technology-related links shared over the weekend. Apple is still the world's most valuable company, but it may be getting harder to hold onto it\n",
      "Generated Summary: apple has been working on a says\n",
      "\n",
      "[Sample 2]\n",
      "Original Text: TD Ameritrade (NYSE: AMTD ) released its monthly Investment Movement Index on Monday which tracks the buying and selling habits of the firm's more tha...\n",
      "True Summary: Investors bought GoPro, Twitter, Kinder Morgan, Southwest Airlines. TD Ameritrade clients were net sellers of Facebook, BofA, Citigroup in Dec.., say analysts. (Closed) The index inched slightly higher in December to 5.20, up from 5.01 in November, says the firm. The firm says it will continue to monitor the buying and selling habits of its millions of clients\n",
      "Generated Summary: apple, amazon, alphabet, amazon all report better-than-expected results. apple, amazon among the stocks to watch on the move on the\n",
      "\n",
      "[Sample 3]\n",
      "Original Text: Halcyon days for tablet sales won't return any time soon, according to a report Monday from Gartner Inc., which forecast unit sales growth of 8 percen...\n",
      "True Summary: Global tablet sales forecast to grow 8 percent this year. Apple, Microsoft and Google will all see their sales grow in the next 12 months: Gartner. (Closed for the week of 30 June) The report is available on the firm's website, as well as through its app store, Twitter and Facebook pages.. Halcyon days for Tablet sales won't return any time soon, according to Gartner\n",
      "Generated Summary: apple to launch new iphone 7 in apple has said it will release a smaller, cheaper version of the iphone in the second half of this year. (no word on when it will be ) apple has said it will release a smaller, cheaper version of the iphone in the second half of this year, but it is not clear if it will be able to keep up with demand\n",
      "\n",
      "[Sample 4]\n",
      "Original Text: Apple Inc. (NASDAQ: AAPL ) shares are trading lower by $0.05 at $106.20 in Tuesday's session.  Earlier today, it found support at $104.63 and rebounde...\n",
      "True Summary: Shares of the tech giant are trading lower by $0.05 at $106.20 in Tuesday's session. Apple has been trading within a narrow range since the end of last week, having traded as low as $104.70 on Oct. 27. The company is scheduled to hold its annual meeting of shareholders on Wednesday, October 31st, at 10:40 am in San Jose, California, in the United States\n",
      "Generated Summary: apple shares are trading lower by in after-hours trading after the company reported better-than-expected iphone sales in the first three months of the year. the company is expected to release its first iphone 7 and 7 plus models in the second half of this year. the company is expected to release new iphone 7 and 7 plus models in the second half of this year, with a price tag of about\n",
      "\n",
      "[Sample 5]\n",
      "Original Text: Apple Inc. (NASDAQ: AAPL ) shares are trading higher by $2.94 at $110.69 in Thursday's session. It has yet to reach its yearly high of $111.44 made on...\n",
      "True Summary: Shares of the tech giant are trading higher by $2.90 on Thursday. Apple has yet to reach its yearly high of $111.44 made on January 2.., but it is expected to do so in the next few weeks, says tech analyst. The world's most valuable company is due to release its latest iPhone on Friday. to host its annual Worldwide Developers Conference in San Jose, California\n",
      "Generated Summary: shares of the tech giant are trading lower by more than 20% in after-hours trading on friday. apple has been trading in a narrow range since the start of the year. (nomura, bofa/merrill, goldman sachs all have similar apple is expected to report better-than-expected earnings next but it is not clear if it will be able to keep up with demand\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "trainer.load_checkpoint('models/best_model.pt')\n",
    "num_samples = min(5, len(val_data))\n",
    "for i in range(num_samples):\n",
    "    src_text = val_data.iloc[i]['clean_body']\n",
    "    true_summary = val_data.iloc[i]['body_summary']\n",
    "        \n",
    "    src_tokens = tokenizer.encode(src_text)[:512]\n",
    "    src_pad_len = 512 - len(src_tokens)\n",
    "    src_tensor = torch.tensor(src_tokens + [0] * src_pad_len, dtype=torch.long)\n",
    "        \n",
    "    generated_tokens = trainer.generate_summary(src_tensor, max_len=128)\n",
    "    generated_summary = tokenizer.decode(generated_tokens.cpu().numpy())\n",
    "        \n",
    "    print(f\"\\n[Sample {i+1}]\")\n",
    "    print(f\"Original Text: {src_text[:150]}...\")\n",
    "    print(f\"True Summary: {true_summary}\")\n",
    "    print(f\"Generated Summary: {generated_summary}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
