{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d505ab7c",
   "metadata": {},
   "source": [
    "## LSTM Multi-Head para Predicci√≥n de Acciones\n",
    "\n",
    "Pipeline completo de entrenamiento LSTM con arquitectura multi-cabeza que predice:\n",
    "- **LOG_RETURN**: retornos logar√≠tmicos (momentum/tendencia)\n",
    "- **ABS_LOG_RETURN**: retornos logar√≠tmicos absolutos (magnitud)\n",
    "- **VOLATILITY**: volatilidad rodante (r√©gimen de mercado)\n",
    "\n",
    "**T√©cnicas anti-overfitting:**\n",
    "- Dropout entre capas LSTM\n",
    "- Regularizaci√≥n L2\n",
    "- Early stopping\n",
    "- Batch normalization\n",
    "- Gradient clipping\n",
    "\n",
    "**Divisi√≥n temporal:**\n",
    "- Train: 2015-2021\n",
    "- Validation: 2021-2023\n",
    "- Test: 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d44955",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas\n",
    "DATA_PATH = '../data/stocks/processed/'\n",
    "RESULTS_PATH = '../results/lstm/'\n",
    "PLOTS_PATH = '../plots/lstm/'\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "\n",
    "TICKERS = ['GOOGL', 'AAPL', 'AMZN', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
    "\n",
    "# Hiperpar√°metros del modelo\n",
    "SEQUENCE_LENGTH = 20\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "L2_REG = 1e-3\n",
    "\n",
    "# Hiperpar√°metros de entrenamiento\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 200\n",
    "PATIENCE = 25\n",
    "\n",
    "# Pesos para p√©rdida ponderada (para embeddings balanceados)\n",
    "TARGET_WEIGHTS = {\n",
    "    'LOG_RETURN': 3.0,\n",
    "    'ABS_LOG_RETURN': 2.0,\n",
    "    'VOLATILITY': 1.0\n",
    "}\n",
    "\n",
    "# Divisiones temporales\n",
    "TRAIN_START = 2015\n",
    "TRAIN_END = 2021\n",
    "VAL_START = 2021\n",
    "VAL_END = 2023\n",
    "TEST_YEAR = 2024\n",
    "\n",
    "# Variables objetivo\n",
    "TARGETS = ['LOG_RETURN', 'ABS_LOG_RETURN', 'VOLATILITY']\n",
    "\n",
    "# Caracter√≠sticas a usar\n",
    "FEATURE_COLS = [\n",
    "    'Close', 'High', 'Low', 'Open', 'Volume',\n",
    "    'SMA_10', 'SMA_20', 'SMA_30',\n",
    "    'UPPER_BAND', 'MIDDLE_BAND', 'LOWER_BAND',\n",
    "    'MACD', 'MACD_SIGNAL', 'MACD_HIST',\n",
    "    'RSI_14',\n",
    "    'STOCH_K', 'STOCH_D',\n",
    "    'WILLIAMS_R',\n",
    "    'LOG_RETURN_HIGH', 'LOG_RETURN_LOW', 'LOG_RETURN_OPEN', 'LOG_RETURN_CLOSE',\n",
    "    'REALIZED_VOL', 'PARKINSON_VOL', 'GARMAN_KLASS_VOL', 'ROGERS_SATCHELL_VOL',\n",
    "    'VWAP',\n",
    "    'DAY_OF_WEEK_SIN', 'DAY_OF_WEEK_COS',\n",
    "    'MONTH_SIN', 'MONTH_COS',\n",
    "    'DAY_OF_MONTH_SIN', 'DAY_OF_MONTH_COS',\n",
    "    'QUARTER_SIN', 'QUARTER_COS'\n",
    "]\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a1dec",
   "metadata": {},
   "source": [
    "### Carga y Preparaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(ticker):\n",
    "    \"\"\"Carga datos y divide por rangos temporales\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Loading {ticker}...\")\n",
    "    print('='*80)\n",
    "    \n",
    "    filepath = os.path.join(DATA_PATH, f\"{ticker}_data_processed.parquet\")\n",
    "    df = pd.read_parquet(filepath)\n",
    "    \n",
    "    # Asegurar columna Date\n",
    "    if 'Date' not in df.columns and df.index.name == 'Date':\n",
    "        df = df.reset_index()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # A√±adir columna de a√±o\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    \n",
    "    # Verificar columnas requeridas\n",
    "    missing_features = [f for f in FEATURE_COLS if f not in df.columns]\n",
    "    missing_targets = [t for t in TARGETS if t not in df.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"  Missing features: {missing_features[:5]}...\")\n",
    "    if missing_targets:\n",
    "        print(f\"  Missing targets: {missing_targets}\")\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    # Seleccionar solo caracter√≠sticas disponibles\n",
    "    available_features = [f for f in FEATURE_COLS if f in df.columns]\n",
    "    \n",
    "    # Divisiones temporales\n",
    "    train_df = df[(df['Year'] >= TRAIN_START) & (df['Year'] <= TRAIN_END)].reset_index(drop=True)\n",
    "    val_df = df[(df['Year'] >= VAL_START) & (df['Year'] <= VAL_END)].reset_index(drop=True)\n",
    "    test_df = df[df['Year'] == TEST_YEAR].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Train: {len(train_df)} samples ({TRAIN_START}-{TRAIN_END})\")\n",
    "    print(f\"  Val:   {len(val_df)} samples ({VAL_START}-{VAL_END})\")\n",
    "    print(f\"  Test:  {len(test_df)} samples ({TEST_YEAR})\")\n",
    "    print(f\"  Features: {len(available_features)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df, available_features, df['Date'], df\n",
    "\n",
    "\n",
    "def create_sequences(data, features, targets, seq_length):\n",
    "    \"\"\"Crea secuencias para entrada LSTM\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[features].iloc[i:i+seq_length].values)\n",
    "        y.append(data[targets].iloc[i+seq_length].values)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    \"\"\"Dataset de PyTorch para secuencias de acciones\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bd83e",
   "metadata": {},
   "source": [
    "### Arquitectura del Modelo LSTM Multi-Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockLSTMMultiHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo LSTM con arquitectura multi-cabeza para generaci√≥n de embeddings\n",
    "    \n",
    "    Arquitectura:\n",
    "      [Input] ‚Üí [LSTM Encoder] ‚Üí [EMBEDDING] ‚Üí [3 cabezas especializadas]\n",
    "      \n",
    "    El encoder aprende una representaci√≥n compartida (embedding) que captura:\n",
    "      - Momentum/tendencia (LOG_RETURN)\n",
    "      - Magnitud (ABS_LOG_RETURN)\n",
    "      - R√©gimen de volatilidad (VOLATILITY)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):\n",
    "        super(StockLSTMMultiHead, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # ===== ENCODER COMPARTIDO =====\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # ===== CABEZAS ESPECIALIZADAS =====\n",
    "        # Cabeza 1: LOG_RETURN (momentum/tendencia)\n",
    "        self.head1_fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.head1_fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "        \n",
    "        # Cabeza 2: ABS_LOG_RETURN (magnitud)\n",
    "        self.head2_fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.head2_fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "        \n",
    "        # Cabeza 3: VOLATILITY (r√©gimen)\n",
    "        self.head3_fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.head3_fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        \"\"\"Forward pass con opci√≥n de retornar embedding\"\"\"\n",
    "        # LSTM encoder\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Tomar √∫ltima salida como embedding\n",
    "        embedding = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Batch norm + dropout\n",
    "        embedding_norm = self.batch_norm(embedding)\n",
    "        embedding_dropped = self.dropout(embedding_norm)\n",
    "        \n",
    "        # Pasar por cada cabeza especializada\n",
    "        h1 = self.relu(self.head1_fc1(embedding_dropped))\n",
    "        out1 = self.head1_fc2(h1)\n",
    "        \n",
    "        h2 = self.relu(self.head2_fc1(embedding_dropped))\n",
    "        out2 = self.head2_fc2(h2)\n",
    "        \n",
    "        h3 = self.relu(self.head3_fc1(embedding_dropped))\n",
    "        out3 = self.head3_fc2(h3)\n",
    "        \n",
    "        # Concatenar salidas\n",
    "        outputs = torch.cat([out1, out2, out3], dim=1)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return outputs, embedding\n",
    "        return outputs\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"Extraer embedding sin calcular predicciones\"\"\"\n",
    "        with torch.no_grad():\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            embedding = lstm_out[:, -1, :]\n",
    "            return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f656e",
   "metadata": {},
   "source": [
    "### Funci√≥n de P√©rdida Ponderada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    P√©rdida MSE ponderada para aprendizaje multi-tarea\n",
    "    \n",
    "    Aplica diferentes pesos a cada objetivo para balancear su contribuci√≥n\n",
    "    a la p√©rdida total. Crucial para el aprendizaje de embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "        self.weights = torch.tensor(list(weights.values()), dtype=torch.float32)\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        # Mover pesos al mismo dispositivo\n",
    "        if self.weights.device != predictions.device:\n",
    "            self.weights = self.weights.to(predictions.device)\n",
    "        \n",
    "        # Calcular MSE por objetivo\n",
    "        mse_per_target = torch.mean((predictions - targets) ** 2, dim=0)\n",
    "        \n",
    "        # Aplicar pesos\n",
    "        weighted_loss = torch.sum(self.weights * mse_per_target)\n",
    "        \n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935fe69",
   "metadata": {},
   "source": [
    "### Funci√≥n de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, patience):\n",
    "    \"\"\"Entrena con early stopping y learning rate scheduling\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Scheduler de learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Training started...\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Initial LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validaci√≥n\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(DEVICE)\n",
    "                y_batch = y_batch.to(DEVICE)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Actualizar learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Imprimir progreso\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch+1}\")\n",
    "            print(f\"  Best validation loss: {best_val_loss:.6f}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb63b1",
   "metadata": {},
   "source": [
    "### Funciones de Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, dataset_name):\n",
    "    \"\"\"Eval√∫a modelo y calcula m√©tricas\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            outputs = model(X_batch)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(y_batch.numpy())\n",
    "    \n",
    "    preds = np.vstack(all_preds)\n",
    "    targets = np.vstack(all_targets)\n",
    "    \n",
    "    # Calcular m√©tricas para cada objetivo\n",
    "    metrics = {}\n",
    "    for i, target_name in enumerate(TARGETS):\n",
    "        mse = mean_squared_error(targets[:, i], preds[:, i])\n",
    "        mae = mean_absolute_error(targets[:, i], preds[:, i])\n",
    "        r2 = r2_score(targets[:, i], preds[:, i])\n",
    "        \n",
    "        metrics[target_name] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': np.sqrt(mse),\n",
    "            'MAE': mae,\n",
    "            'R2': r2\n",
    "        }\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"\\nüìä {dataset_name} Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    for target_name, target_metrics in metrics.items():\n",
    "        print(f\"  {target_name}:\")\n",
    "        for metric_name, value in target_metrics.items():\n",
    "            print(f\"    {metric_name}: {value:.6f}\")\n",
    "    \n",
    "    return metrics, preds, targets\n",
    "\n",
    "\n",
    "def plot_predictions(preds, targets, ticker, dataset_name, dates_subset=None):\n",
    "    \"\"\"Grafica predicciones vs reales para cada objetivo\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    for i, target_name in enumerate(TARGETS):\n",
    "        ax = axes[i]\n",
    "        x = dates_subset if dates_subset is not None else range(len(targets))\n",
    "        \n",
    "        ax.plot(x, targets[:, i], label='Actual', alpha=0.7)\n",
    "        ax.plot(x, preds[:, i], label='Predicted', alpha=0.7)\n",
    "        ax.set_title(f'{ticker} - {target_name} ({dataset_name})')\n",
    "        ax.set_ylabel(target_name)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time' if dates_subset is None else 'Date')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plot_path = os.path.join(PLOTS_PATH, f\"{ticker}_{dataset_name}_predictions.png\")\n",
    "    fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  üìà Saved plot: {plot_path}\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc2b2c",
   "metadata": {},
   "source": [
    "### Extracci√≥n de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323346d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, dataloader, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Extrae embeddings del modelo entrenado\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: array [num_samples, hidden_size]\n",
    "        targets: array [num_samples, num_targets]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            embeddings = model.get_embedding(X_batch)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            all_targets.append(y_batch.numpy())\n",
    "    \n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    targets = np.vstack(all_targets)\n",
    "    \n",
    "    return embeddings, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe529a",
   "metadata": {},
   "source": [
    "### Pipeline Principal por Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_for_ticker(ticker):\n",
    "    \"\"\"Pipeline completo para un ticker con normalizaci√≥n de targets\"\"\"\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# Processing {ticker}\")\n",
    "    print('#'*80)\n",
    "    \n",
    "    # Cargar datos\n",
    "    train_df, val_df, test_df, features, dates, full_df = load_and_split_data(ticker)\n",
    "    if train_df is None:\n",
    "        print(f\"Skipping {ticker} due to missing data\")\n",
    "        return None\n",
    "    \n",
    "    # Escalar features\n",
    "    print(\"\\nScaling features...\")\n",
    "    feature_scaler = RobustScaler()\n",
    "    train_df[features] = feature_scaler.fit_transform(train_df[features])\n",
    "    val_df[features] = feature_scaler.transform(val_df[features])\n",
    "    test_df[features] = feature_scaler.transform(test_df[features])\n",
    "    \n",
    "    # Escalar targets individualmente\n",
    "    print(\"Scaling targets individually...\")\n",
    "    target_scalers = {}\n",
    "    for target in TARGETS:\n",
    "        scaler = RobustScaler()\n",
    "        train_df[[target]] = scaler.fit_transform(train_df[[target]])\n",
    "        val_df[[target]] = scaler.transform(val_df[[target]])\n",
    "        test_df[[target]] = scaler.transform(test_df[[target]])\n",
    "        target_scalers[target] = scaler\n",
    "        print(f\"  {target}: median={scaler.center_[0]:.6f}, scale={scaler.scale_[0]:.6f}\")\n",
    "    \n",
    "    # Crear secuencias\n",
    "    print(\"\\nCreating sequences...\")\n",
    "    X_train, y_train = create_sequences(train_df, features, TARGETS, SEQUENCE_LENGTH)\n",
    "    X_val, y_val = create_sequences(val_df, features, TARGETS, SEQUENCE_LENGTH)\n",
    "    X_test, y_test = create_sequences(test_df, features, TARGETS, SEQUENCE_LENGTH)\n",
    "    \n",
    "    print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"  X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "    print(f\"  X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
    "    \n",
    "    # Crear DataLoaders\n",
    "    train_dataset = StockDataset(X_train, y_train)\n",
    "    val_dataset = StockDataset(X_val, y_val)\n",
    "    test_dataset = StockDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Inicializar modelo\n",
    "    input_size = len(features)\n",
    "    output_size = len(TARGETS)\n",
    "    \n",
    "    model = StockLSTMMultiHead(\n",
    "        input_size=input_size,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_size=output_size,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nModel: {total_params:,} parameters\")\n",
    "    \n",
    "    # Loss y optimizer\n",
    "    criterion = WeightedMSELoss(TARGET_WEIGHTS)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)\n",
    "    \n",
    "    print(f\"\\nLoss weights: {TARGET_WEIGHTS}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    model, train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE\n",
    "    )\n",
    "    \n",
    "    # Evaluar\n",
    "    train_metrics, train_preds, train_targets = evaluate_model(model, train_loader, \"TRAIN\")\n",
    "    val_metrics, val_preds, val_targets = evaluate_model(model, val_loader, \"VALIDATION\")\n",
    "    test_metrics, test_preds, test_targets = evaluate_model(model, test_loader, \"TEST\")\n",
    "    \n",
    "    # Desnormalizar predicciones\n",
    "    print(\"\\Denormalizing predictions...\")\n",
    "    for i, target in enumerate(TARGETS):\n",
    "        scaler = target_scalers[target]\n",
    "        train_preds[:, i] = scaler.inverse_transform(train_preds[:, i].reshape(-1, 1)).flatten()\n",
    "        train_targets[:, i] = scaler.inverse_transform(train_targets[:, i].reshape(-1, 1)).flatten()\n",
    "        \n",
    "        val_preds[:, i] = scaler.inverse_transform(val_preds[:, i].reshape(-1, 1)).flatten()\n",
    "        val_targets[:, i] = scaler.inverse_transform(val_targets[:, i].reshape(-1, 1)).flatten()\n",
    "        \n",
    "        test_preds[:, i] = scaler.inverse_transform(test_preds[:, i].reshape(-1, 1)).flatten()\n",
    "        test_targets[:, i] = scaler.inverse_transform(test_targets[:, i].reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Recalcular m√©tricas\n",
    "    print(\"\\nFINAL METRICS (Denormalized):\")\n",
    "    train_metrics_final = {}\n",
    "    val_metrics_final = {}\n",
    "    test_metrics_final = {}\n",
    "    \n",
    "    for i, target in enumerate(TARGETS):\n",
    "        for metrics_dict, preds, targets, name in [\n",
    "            (train_metrics_final, train_preds, train_targets, \"TRAIN\"),\n",
    "            (val_metrics_final, val_preds, val_targets, \"VALIDATION\"),\n",
    "            (test_metrics_final, test_preds, test_targets, \"TEST\")\n",
    "        ]:\n",
    "            mse = mean_squared_error(targets[:, i], preds[:, i])\n",
    "            mae = mean_absolute_error(targets[:, i], preds[:, i])\n",
    "            r2 = r2_score(targets[:, i], preds[:, i])\n",
    "            \n",
    "            metrics_dict[target] = {\n",
    "                'MSE': mse,\n",
    "                'RMSE': np.sqrt(mse),\n",
    "                'MAE': mae,\n",
    "                'R2': r2\n",
    "            }\n",
    "    \n",
    "    # Imprimir m√©tricas finales\n",
    "    for name, metrics_dict in [(\"TRAIN\", train_metrics_final), (\"VALIDATION\", val_metrics_final), (\"TEST\", test_metrics_final)]:\n",
    "        print(f\"\\n  {name}:\")\n",
    "        for target, target_metrics in metrics_dict.items():\n",
    "            print(f\"    {target}: R¬≤={target_metrics['R2']:.4f}, RMSE={target_metrics['RMSE']:.6f}\")\n",
    "    \n",
    "    # Obtener fechas para gr√°ficos\n",
    "    train_dates = train_df['Date'].iloc[SEQUENCE_LENGTH:].values\n",
    "    val_dates = val_df['Date'].iloc[SEQUENCE_LENGTH:].values\n",
    "    test_dates = test_df['Date'].iloc[SEQUENCE_LENGTH:].values\n",
    "    \n",
    "    # Graficar predicciones\n",
    "    plot_predictions(train_preds, train_targets, ticker, \"train\", train_dates)\n",
    "    plot_predictions(val_preds, val_targets, ticker, \"validation\", val_dates)\n",
    "    plot_predictions(test_preds, test_targets, ticker, \"test\", test_dates)\n",
    "    \n",
    "    # Graficar curvas de entrenamiento\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(train_losses, label='Train Loss')\n",
    "    ax.plot(val_losses, label='Val Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Weighted Loss')\n",
    "    ax.set_title(f'{ticker} - Training Curves')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    curve_path = os.path.join(PLOTS_PATH, f\"{ticker}_training_curves.png\")\n",
    "    fig.savefig(curve_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved training curves: {curve_path}\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Guardar m√©tricas a CSV\n",
    "    results = []\n",
    "    for dataset_name, metrics in [('train', train_metrics_final), ('validation', val_metrics_final), ('test', test_metrics_final)]:\n",
    "        for target_name, target_metrics in metrics.items():\n",
    "            row = {\n",
    "                'ticker': ticker,\n",
    "                'dataset': dataset_name,\n",
    "                'target': target_name,\n",
    "                **target_metrics\n",
    "            }\n",
    "            results.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_path = os.path.join(RESULTS_PATH, f\"{ticker}_metrics.csv\")\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"Saved metrics: {results_path}\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    model_path = os.path.join(RESULTS_PATH, f\"{ticker}_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'features': features,\n",
    "        'feature_scaler': feature_scaler,\n",
    "        'target_scalers': target_scalers,\n",
    "        'target_weights': TARGET_WEIGHTS,\n",
    "        'config': {\n",
    "            'input_size': input_size,\n",
    "            'hidden_size': HIDDEN_SIZE,\n",
    "            'num_layers': NUM_LAYERS,\n",
    "            'output_size': output_size,\n",
    "            'dropout': DROPOUT,\n",
    "            'architecture': 'StockLSTMMultiHead'\n",
    "        }\n",
    "    }, model_path)\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "    \n",
    "    # Extraer y guardar embeddings\n",
    "    print(\"\\nExtracting embeddings...\")\n",
    "    embeddings_path = '../data/embeddings/lstm_multihead/'\n",
    "    os.makedirs(embeddings_path, exist_ok=True)\n",
    "    \n",
    "    train_embeddings, _ = extract_embeddings(model, train_loader, DEVICE)\n",
    "    val_embeddings, _ = extract_embeddings(model, val_loader, DEVICE)\n",
    "    test_embeddings, _ = extract_embeddings(model, test_loader, DEVICE)\n",
    "    \n",
    "    print(f\"  Train embeddings: {train_embeddings.shape}\")\n",
    "    print(f\"  Val embeddings: {val_embeddings.shape}\") \n",
    "    print(f\"  Test embeddings: {test_embeddings.shape}\")\n",
    "    \n",
    "    # Guardar embeddings\n",
    "    for split_name, embeddings, targets in [\n",
    "        ('train', train_embeddings, train_targets),\n",
    "        ('val', val_embeddings, val_targets),\n",
    "        ('test', test_embeddings, test_targets)\n",
    "    ]:\n",
    "        emb_path = os.path.join(embeddings_path, f\"{ticker}_{split_name}_embeddings.npz\")\n",
    "        np.savez_compressed(emb_path, embeddings=embeddings, targets=targets)\n",
    "        print(f\"Saved: {emb_path}\")\n",
    "    \n",
    "    print(f\"\\n{ticker} processing complete!\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c1334",
   "metadata": {},
   "source": [
    "### Ejecutar Pipeline para Todos los Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b772024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LSTM Multi-Head Stock Embedding & Prediction Pipeline\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Targets: {TARGETS}\")\n",
    "print(f\"Target Weights: {TARGET_WEIGHTS}\")\n",
    "print(f\"Train: {TRAIN_START}-{TRAIN_END}\")\n",
    "print(f\"Val: {VAL_START}-{VAL_END}\")\n",
    "print(f\"Test: {TEST_YEAR}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    try:\n",
    "        results = run_lstm_for_ticker(ticker)\n",
    "        if results is not None:\n",
    "            all_results.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {ticker}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846dc6c",
   "metadata": {},
   "source": [
    "### Resultados Combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los resultados\n",
    "if all_results:\n",
    "    combined_results = pd.concat(all_results, ignore_index=True)\n",
    "    combined_path = os.path.join(RESULTS_PATH, \"all_tickers_metrics.csv\")\n",
    "    combined_results.to_csv(combined_path, index=False)\n",
    "    print(f\"\\nCombined metrics saved: {combined_path}\")\n",
    "    \n",
    "    # Estad√≠sticas resumen\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Summary Statistics (Test Set)\")\n",
    "    print(\"=\"*80)\n",
    "    test_results = combined_results[combined_results['dataset'] == 'test']\n",
    "    summary = test_results.groupby('target')[['MSE', 'RMSE', 'MAE', 'R2']].mean()\n",
    "    display(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Pipeline completed!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
