{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd3eb34",
   "metadata": {},
   "source": [
    "# Embeddings No Contextuales con FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3bf2d",
   "metadata": {},
   "source": [
    "En este notebook se generan embeddings no contextuales para las noticias utilizando FastText. Los embeddings se obtienen promediando los vectores de palabras de cada texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087cd9a",
   "metadata": {},
   "source": [
    "Importar todas las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14cc67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21b276",
   "metadata": {},
   "source": [
    "Descargar recursos necesarios de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff3355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/adiez/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1df6ab",
   "metadata": {},
   "source": [
    "Definir directorios de donde queremos cargar los datos resumidos y donde queremos guardar los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d48031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han encontrado 7 archivos parquet para procesar.\n",
      "\n",
      "amazon_news.parquet\n",
      "apple_news.parquet\n",
      "google_news.parquet\n",
      "meta_news.parquet\n",
      "microsoft_news.parquet\n",
      "nvidia_news.parquet\n",
      "tesla_news.parquet\n"
     ]
    }
   ],
   "source": [
    "# Usa el directorio actual del notebook como raíz del proyecto\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Define las rutas a los directorios de datos\n",
    "SUMMARIES_DIR = os.path.join(ROOT_DIR, 'data', 'news', 'summarized')\n",
    "FASTTEXT_PATH = os.path.join(ROOT_DIR, 'models', 'cc.en.300.bin')\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'data', 'news', 'embeddings')\n",
    "\n",
    "# Crea el directorio de salida si no existe\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Lista todos los archivos .parquet en el directorio SUMMARIES_DIR\n",
    "parquet_files = [f for f in os.listdir(SUMMARIES_DIR) if f.endswith('.parquet')]\n",
    "\n",
    "# Muestra el número total de archivos encontrados\n",
    "print(f\"Se han encontrado {len(parquet_files)} archivos parquet para procesar.\\n\")\n",
    "\n",
    "# Lista los nombres de los archivos encontrados\n",
    "for file in parquet_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba85b7",
   "metadata": {},
   "source": [
    "Cargar modelo preentrenado de FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50f773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo FastText cargado.\n",
      "Dimensión de embeddings: 300\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo FastText\n",
    "ft = fasttext.load_model(FASTTEXT_PATH)\n",
    "print(\"Modelo FastText cargado.\")\n",
    "print(f\"Dimensión de embeddings: {ft.get_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604d945",
   "metadata": {},
   "source": [
    "Definir función para generar embeddings de texto promediando vectores de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19008865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(tokens):\n",
    "    \"\"\"\n",
    "    Genera embedding de texto promediando los vectores de palabras.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        embeddings.append(ft.get_word_vector(token))\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(ft.get_dimension())\n",
    "    \n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067489c",
   "metadata": {},
   "source": [
    "Procesamiento de archivos: para cada archivo se realiza la carga de datos, concatenación de título y resumen, tokenización, generación de embeddings y guardado de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2c95a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando amazon_news.parquet...\n",
      "  Guardado amazon_embeddings_no_context.parquet con 12391 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando apple_news.parquet...\n",
      "  Guardado amazon_embeddings_no_context.parquet con 12391 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando apple_news.parquet...\n",
      "  Guardado apple_embeddings_no_context.parquet con 24208 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando google_news.parquet...\n",
      "  Guardado apple_embeddings_no_context.parquet con 24208 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando google_news.parquet...\n",
      "  Guardado google_embeddings_no_context.parquet con 13341 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando meta_news.parquet...\n",
      "  Guardado google_embeddings_no_context.parquet con 13341 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando meta_news.parquet...\n",
      "  Guardado meta_embeddings_no_context.parquet con 15187 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando microsoft_news.parquet...\n",
      "  Guardado meta_embeddings_no_context.parquet con 15187 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando microsoft_news.parquet...\n",
      "  Guardado microsoft_embeddings_no_context.parquet con 12931 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando nvidia_news.parquet...\n",
      "  Guardado microsoft_embeddings_no_context.parquet con 12931 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando nvidia_news.parquet...\n",
      "  Guardado nvidia_embeddings_no_context.parquet con 10215 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando tesla_news.parquet...\n",
      "  Guardado nvidia_embeddings_no_context.parquet con 10215 registros y dimensión de embedding 300\n",
      "\n",
      "Procesando tesla_news.parquet...\n",
      "  Guardado tesla_embeddings_no_context.parquet con 27131 registros y dimensión de embedding 300\n",
      "\n",
      "Todos los archivos han sido procesados exitosamente!\n",
      "Archivos guardados en: /home/adiez/Desktop/News2Stock/data/news/embeddings\n",
      "  Guardado tesla_embeddings_no_context.parquet con 27131 registros y dimensión de embedding 300\n",
      "\n",
      "Todos los archivos han sido procesados exitosamente!\n",
      "Archivos guardados en: /home/adiez/Desktop/News2Stock/data/news/embeddings\n"
     ]
    }
   ],
   "source": [
    "for file_name in parquet_files:\n",
    "    print(f\"Procesando {file_name}...\")\n",
    "    \n",
    "    # Cargar dataset\n",
    "    df = pd.read_parquet(os.path.join(SUMMARIES_DIR, file_name), engine=\"pyarrow\")\n",
    "    \n",
    "    # Concatenar título y resumen\n",
    "    df[\"text\"] = df[\"title\"] + \"\\n\\n\" + df[\"body_summary\"]\n",
    "    df = df.drop(columns=[\"clean_body\", \"body_summary\", \"teaser\", \"title\", \"author\"])\n",
    "    \n",
    "    # Eliminar filas con texto vacío\n",
    "    df = df.dropna(subset=['text'])\n",
    "    \n",
    "    # Tokenizar la columna 'text'\n",
    "    df['tokens'] = df['text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "    \n",
    "    # Generar embeddings\n",
    "    df['embedding'] = df['tokens'].apply(get_text_embedding)\n",
    "    \n",
    "    # Eliminar columna de tokens\n",
    "    df = df.drop(columns=['tokens'])\n",
    "    \n",
    "    # Guardar en directorio de salida\n",
    "    output_name = file_name.replace('_news.parquet', '_embeddings_no_context.parquet')\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    df.to_parquet(output_path, engine=\"pyarrow\", index=False)\n",
    "    \n",
    "    print(f\"  Guardado {output_name} con {len(df)} registros y dimensión de embedding {ft.get_dimension()}\\n\")\n",
    "\n",
    "print(\"Todos los archivos han sido procesados exitosamente!\")\n",
    "print(\"Archivos guardados en:\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
