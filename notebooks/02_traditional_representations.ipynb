{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c1b392",
   "metadata": {},
   "source": [
    "# Representaciones Tradicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a7ac0",
   "metadata": {},
   "source": [
    "En este notebook se generan representaciones tradicionales de texto para las noticias ya resumidas. Se aplicarán dos técnicas principales: Bag of Words (BoW) y TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec69ed",
   "metadata": {},
   "source": [
    "Importar todas las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601f8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15421ff",
   "metadata": {},
   "source": [
    "Definir directorios de donde queremos cargar los datos resumidos y donde queremos guardar las representaciones tradicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc044bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han encontrado 7 archivos parquet para procesar.\n",
      "\n",
      "amazon_news.parquet\n",
      "apple_news.parquet\n",
      "google_news.parquet\n",
      "meta_news.parquet\n",
      "microsoft_news.parquet\n",
      "nvidia_news.parquet\n",
      "tesla_news.parquet\n"
     ]
    }
   ],
   "source": [
    "# Usa el directorio actual del notebook como raíz del proyecto\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Define las rutas a los directorios de datos\n",
    "SUMMARIES_DIR = os.path.join(ROOT_DIR, 'data', 'news', 'summarized')\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'data', 'news', 'traditional')\n",
    "\n",
    "# Crea el directorio de salida si no existe\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Lista todos los archivos .parquet en el directorio SUMMARIES_DIR\n",
    "parquet_files = [f for f in os.listdir(SUMMARIES_DIR) if f.endswith('.parquet')]\n",
    "\n",
    "# Muestra el número total de archivos encontrados\n",
    "print(f\"Se han encontrado {len(parquet_files)} archivos parquet para procesar.\\n\")\n",
    "\n",
    "# Lista los nombres de los archivos encontrados\n",
    "for file in parquet_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875db988",
   "metadata": {},
   "source": [
    "Cargar el modelo de spaCy para procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b1bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo de spaCy...\n",
      "Modelo de spaCy cargado.\n",
      "Modelo de spaCy cargado.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando modelo de spaCy...\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # Para instalar: python -m spacy download en_core_web_lg\n",
    "print(\"Modelo de spaCy cargado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de24e1",
   "metadata": {},
   "source": [
    "Definir funciones de preprocesamiento de texto: limpieza básica (minúsculas, eliminación de puntuación y números) y preprocesamiento completo (lematización y eliminación de stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d359b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpia el texto: minúsculas, eliminar puntuación y números.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Limpia y lematiza el texto, eliminando stopwords.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.lemma_ != '-PRON-']\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c671eca",
   "metadata": {},
   "source": [
    "Definir el tamaño de lote para procesamiento en batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98d0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f886b",
   "metadata": {},
   "source": [
    "Procesamiento de archivos: para cada archivo se realiza la carga de datos, concatenación de título y resumen, preprocesamiento del texto, generación de representaciones Bag of Words y TF-IDF, y guardado de resultados junto con los vectorizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5416c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] Procesando amazon_news.parquet...\n",
      "  Preprocesando 12391 textos en lotes de 500...\n",
      "    Progreso: 12391/12391 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Progreso: 12391/12391 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Lote 25/25 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 25/25 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 25/25 procesado\n",
      "    Lote 25/25 procesado\n",
      "  Guardado amazon_traditional.parquet con 12391 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[2/7] Procesando apple_news.parquet...\n",
      "  Guardado amazon_traditional.parquet con 12391 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[2/7] Procesando apple_news.parquet...\n",
      "  Preprocesando 24208 textos en lotes de 500...\n",
      "  Preprocesando 24208 textos en lotes de 500...\n",
      "    Progreso: 24208/24208 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Progreso: 24208/24208 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Lote 49/49 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 49/49 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 49/49 procesado\n",
      "    Lote 49/49 procesado\n",
      "  Guardado apple_traditional.parquet con 24208 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[3/7] Procesando google_news.parquet...\n",
      "  Preprocesando 13341 textos en lotes de 500...\n",
      "  Guardado apple_traditional.parquet con 24208 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[3/7] Procesando google_news.parquet...\n",
      "  Preprocesando 13341 textos en lotes de 500...\n",
      "    Progreso: 13341/13341 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Progreso: 13341/13341 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Lote 27/27 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 27/27 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 27/27 procesado\n",
      "    Lote 27/27 procesado\n",
      "  Guardado google_traditional.parquet con 13341 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[4/7] Procesando meta_news.parquet...\n",
      "  Preprocesando 15187 textos en lotes de 500...\n",
      "  Guardado google_traditional.parquet con 13341 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[4/7] Procesando meta_news.parquet...\n",
      "  Preprocesando 15187 textos en lotes de 500...\n",
      "    Progreso: 15187/15187 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Progreso: 15187/15187 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Lote 31/31 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 31/31 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 31/31 procesado\n",
      "    Lote 31/31 procesado\n",
      "  Guardado meta_traditional.parquet con 15187 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[5/7] Procesando microsoft_news.parquet...\n",
      "  Se han eliminado 1 filas con texto vacío\n",
      "  Preprocesando 12931 textos en lotes de 500...\n",
      "  Guardado meta_traditional.parquet con 15187 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[5/7] Procesando microsoft_news.parquet...\n",
      "  Se han eliminado 1 filas con texto vacío\n",
      "  Preprocesando 12931 textos en lotes de 500...\n",
      "    Progreso: 12931/12931 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Progreso: 12931/12931 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Lote 26/26 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 26/26 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 26/26 procesado\n",
      "    Lote 26/26 procesado\n",
      "  Guardado microsoft_traditional.parquet con 12931 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[6/7] Procesando nvidia_news.parquet...\n",
      "  Preprocesando 10215 textos en lotes de 500...\n",
      "  Guardado microsoft_traditional.parquet con 12931 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[6/7] Procesando nvidia_news.parquet...\n",
      "  Preprocesando 10215 textos en lotes de 500...\n",
      "    Progreso: 10215/10215 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Progreso: 10215/10215 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Lote 21/21 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 21/21 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 21/21 procesado\n",
      "    Lote 21/21 procesado\n",
      "  Guardado nvidia_traditional.parquet con 10215 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[7/7] Procesando tesla_news.parquet...\n",
      "  Guardado nvidia_traditional.parquet con 10215 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "[7/7] Procesando tesla_news.parquet...\n",
      "  Preprocesando 27131 textos en lotes de 500...\n",
      "  Preprocesando 27131 textos en lotes de 500...\n",
      "    Progreso: 27131/27131 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Progreso: 27131/27131 (100.0%)\n",
      "  Generando representación Bag of Words...\n",
      "    Lote 55/55 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 55/55 procesado\n",
      "  Generando representación TF-IDF...\n",
      "    Lote 55/55 procesado\n",
      "    Lote 55/55 procesado\n",
      "  Guardado tesla_traditional.parquet con 27131 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "Todos los archivos han sido procesados exitosamente!\n",
      "Archivos guardados en: /home/adiez/Desktop/News2Stock/data/news/traditional\n",
      "  Guardado tesla_traditional.parquet con 27131 registros\n",
      "    Dimensión BoW: 5000, Dimensión TF-IDF: 5000\n",
      "\n",
      "Todos los archivos han sido procesados exitosamente!\n",
      "Archivos guardados en: /home/adiez/Desktop/News2Stock/data/news/traditional\n"
     ]
    }
   ],
   "source": [
    "for file_idx, file_name in enumerate(parquet_files, 1):\n",
    "    print(f\"[{file_idx}/{len(parquet_files)}] Procesando {file_name}...\")\n",
    "    \n",
    "    # Cargar dataset\n",
    "    df = pd.read_parquet(os.path.join(SUMMARIES_DIR, file_name), engine=\"pyarrow\")\n",
    "    \n",
    "    # Concatenar título y resumen\n",
    "    df[\"text\"] = df[\"title\"] + \"\\n\\n\" + df[\"body_summary\"]\n",
    "    df = df.drop(columns=[\"clean_body\", \"body_summary\", \"teaser\", \"title\", \"author\"])\n",
    "    \n",
    "    # Eliminar filas con texto vacío\n",
    "    original_count = len(df)\n",
    "    df = df.dropna(subset=['text'])\n",
    "    final_count = len(df)\n",
    "    \n",
    "    if original_count != final_count:\n",
    "        print(f\"  Se han eliminado {original_count - final_count} filas con texto vacío\")\n",
    "    \n",
    "    print(f\"  Preprocesando {final_count} textos en lotes de {BATCH_SIZE}...\")\n",
    "    \n",
    "    # Aplicar preprocesamiento en batches\n",
    "    processed_texts = []\n",
    "    num_batches = (final_count + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = min((batch_idx + 1) * BATCH_SIZE, final_count)\n",
    "        \n",
    "        batch_texts = df['text'].iloc[start_idx:end_idx]\n",
    "        \n",
    "        for idx, text in enumerate(batch_texts, start=start_idx + 1):\n",
    "            if idx % 100 == 0 or idx == final_count:\n",
    "                print(f\"    Progreso: {idx}/{final_count} ({100*idx/final_count:.1f}%)\", end='\\r')\n",
    "            processed_texts.append(preprocess_text(text))\n",
    "    \n",
    "    df['text_processed'] = processed_texts\n",
    "    print()  # Nueva línea después del progreso\n",
    "    \n",
    "    # Generar Bag of Words\n",
    "    print(\"  Generando representación Bag of Words...\")\n",
    "    bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "    \n",
    "    # Entrenar vectorizador con todos los datos\n",
    "    bow_vectorizer.fit(df['text_processed'])\n",
    "    \n",
    "    # Transformar en batches\n",
    "    bow_vectors = []\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = min((batch_idx + 1) * BATCH_SIZE, final_count)\n",
    "        \n",
    "        batch_processed = df['text_processed'].iloc[start_idx:end_idx]\n",
    "        batch_bow = bow_vectorizer.transform(batch_processed)\n",
    "        bow_vectors.extend(batch_bow.toarray())\n",
    "        \n",
    "        if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == num_batches:\n",
    "            print(f\"    Lote {batch_idx + 1}/{num_batches} procesado\", end='\\r')\n",
    "    \n",
    "    df['bow'] = bow_vectors\n",
    "    print()  # Nueva línea\n",
    "    \n",
    "    # Generar TF-IDF\n",
    "    print(\"  Generando representación TF-IDF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    \n",
    "    # Entrenar vectorizador con todos los datos\n",
    "    tfidf_vectorizer.fit(df['text_processed'])\n",
    "    \n",
    "    # Transformar en batches\n",
    "    tfidf_vectors = []\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = min((batch_idx + 1) * BATCH_SIZE, final_count)\n",
    "        \n",
    "        batch_processed = df['text_processed'].iloc[start_idx:end_idx]\n",
    "        batch_tfidf = tfidf_vectorizer.transform(batch_processed)\n",
    "        tfidf_vectors.extend(batch_tfidf.toarray())\n",
    "        \n",
    "        if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == num_batches:\n",
    "            print(f\"    Lote {batch_idx + 1}/{num_batches} procesado\", end='\\r')\n",
    "    \n",
    "    df['tfidf'] = tfidf_vectors\n",
    "    print()  # Nueva línea\n",
    "    \n",
    "    # Eliminar columna de texto preprocesado\n",
    "    df = df.drop(columns=['text_processed'])\n",
    "    \n",
    "    # Guardar en directorio de salida\n",
    "    base_name = file_name.replace('_news.parquet', '')\n",
    "    output_name = f\"{base_name}_traditional.parquet\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    df.to_parquet(output_path, engine=\"pyarrow\", index=False)\n",
    "    \n",
    "    # Guardar vectorizadores para uso futuro\n",
    "    bow_vectorizer_path = os.path.join(OUTPUT_DIR, f\"{base_name}_bow_vectorizer.pkl\")\n",
    "    tfidf_vectorizer_path = os.path.join(OUTPUT_DIR, f\"{base_name}_tfidf_vectorizer.pkl\")\n",
    "    \n",
    "    with open(bow_vectorizer_path, 'wb') as f:\n",
    "        pickle.dump(bow_vectorizer, f)\n",
    "    with open(tfidf_vectorizer_path, 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "    bow_dim = len(bow_vectors[0])\n",
    "    tfidf_dim = len(tfidf_vectors[0])\n",
    "    print(f\"  Guardado {output_name} con {len(df)} registros\")\n",
    "    print(f\"    Dimensión BoW: {bow_dim}, Dimensión TF-IDF: {tfidf_dim}\\n\")\n",
    "\n",
    "print(\"Todos los archivos han sido procesados exitosamente!\")\n",
    "print(\"Archivos guardados en:\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
